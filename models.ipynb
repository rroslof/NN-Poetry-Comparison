{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d71639a-55ea-4149-8200-6443d9da4e34",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network Comparison for Poetry Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99683dca-fac7-4941-a82f-bd2d7d6886a8",
   "metadata": {},
   "source": [
    "# 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f1443b-8148-4210-82f0-2105a765e42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General python libraries\n",
    "import math, time, gzip, json, random, re\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader,TensorDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f8d91-18d4-49fb-9fda-1c73a2c9dbfa",
   "metadata": {},
   "source": [
    "# 2. Download the dataset\n",
    "\n",
    "This project uses a corpus of English-language poetry from Project Gutenberg, as curated here: https://github.com/aparrish/gutenberg-poetry-corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aab17763-18e7-49d7-9d94-fe5ef8da0959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poetry data downloaded to file 'data.ndjson'\n"
     ]
    }
   ],
   "source": [
    "url = 'http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz'\n",
    "data_file = 'data.ndjson'\n",
    "\n",
    "with requests.get(url, stream=True) as r:\n",
    "    with open(data_file, 'wb') as f:\n",
    "        shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "print(f\"Poetry data downloaded to file '{data_file}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0a420d-90b1-42bc-9f3d-349657687856",
   "metadata": {},
   "source": [
    "# 3. Preprocessing functions\n",
    "\n",
    "## Load & vectorize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ca3349d-0e9b-4bb8-a2ef-74db18b963be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(data_file, char_limit, line_offset, use_words):\n",
    "    print(\"\\n -= Preprocessing Statistics =- \")\n",
    "    \n",
    "    # Load data from ndjson file.  The character limit prevents crashes from excessive memory usage.\n",
    "    all_lines = []\n",
    "    char_count = 0\n",
    "    line_count = 0\n",
    "    for line in gzip.open(data_file):\n",
    "        if (line_count >= line_offset):\n",
    "            parsed_line = json.loads(line.strip())['s']\n",
    "            all_lines.append(parsed_line)\n",
    "            \n",
    "            char_count += len(parsed_line)\n",
    "            if (char_count > char_limit):\n",
    "                break\n",
    "\n",
    "        line_count += 1\n",
    "        \n",
    "    print(f\"Loaded {len(all_lines)} lines of poetry\")\n",
    "\n",
    "    # Join all lines to create the raw input string\n",
    "    raw_input = \"\\n\".join(all_lines)\n",
    "    print(f\"Raw input length: {len(raw_input)} characters\")\n",
    "\n",
    "    # Sanitize input data\n",
    "    input_data = re.sub(r'[^a-zA-Z0-9_ \\n\\.\\!\\?\\&\\-\\:\\;\\']', '', raw_input)\n",
    "    print(f\"Sanitized input length: {len(input_data)} total characters\")\n",
    "\n",
    "    # If using a word-based model, split words while preserving newlines\n",
    "    if (use_words):\n",
    "        input_data = input_data.replace('\\n', ' \\n ').split(' ')\n",
    "        print(f\"Split input string into {len(input_data)} words\")\n",
    "\n",
    "    # Determine the input vocabulary\n",
    "    vocab = sorted(set(input_data))\n",
    "    print(f\"Vocabulary length: {len(vocab)} distinct tokens\")\n",
    "\n",
    "    # Get token mappings. Used for vectorization, model input/output sizes, and text generation.\n",
    "    token_to_index = {token: index for index, token in enumerate(vocab)}\n",
    "    index_to_token = np.array(vocab)\n",
    "\n",
    "    # Vectorize the input data\n",
    "    vectorized_input = np.array([token_to_index[token] for token in input_data])\n",
    "\n",
    "    return vectorized_input, token_to_index, index_to_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6857e130-925c-4719-948f-7b5a19772cde",
   "metadata": {},
   "source": [
    "## Create training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "faf17550-757a-4780-90b8-05de1d322e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sequences(tokens, sequence_length):\n",
    "    sequence_count = len(tokens) - sequence_length\n",
    "    print(f\"Total training sequences: {sequence_count}\")\n",
    "    \n",
    "    sequences = np.zeros((sequence_count, sequence_length))\n",
    "    targets = np.zeros(sequence_count)\n",
    "    \n",
    "    for i in range(0, sequence_count):\n",
    "        sequence = tokens[i:i + sequence_length]\n",
    "        target = tokens[i + sequence_length]\n",
    "        sequences[i] = sequence\n",
    "        targets[i] = target\n",
    "        # if (i % 10000000 == 0):\n",
    "        #     print(f\"Sequence {i}:  Sequence: {sequence}, Next Token: {target}\")\n",
    "    return sequences, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d01b91-8900-499c-9b4d-5e6c559824d4",
   "metadata": {},
   "source": [
    "## Construct the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f159f487-58c2-4323-bda0-e98e24fbd7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_data_loader(vectorized_input, sequence_length, batch_size):\n",
    "    print(\"\\n -= Data Loader Statistics =- \")\n",
    "    sequences, targets = generate_sequences(vectorized_input, sequence_length)\n",
    "    \n",
    "    inputs_tensor = torch.tensor(sequences, dtype=torch.long)\n",
    "    print(f\"Created input tensor with shape: {inputs_tensor.shape}\")\n",
    "    targets_tensor = torch.tensor(targets, dtype=torch.long)\n",
    "    print(f\"Created target tensor with shape: {targets_tensor.shape}\")\n",
    "    \n",
    "    dataset = TensorDataset(inputs_tensor, targets_tensor)\n",
    "    data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48446f-6d10-4bc2-aacc-6f070a1cd56f",
   "metadata": {},
   "source": [
    "# 4. Parameterized model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2112bba6-81ac-40ef-b815-fc3f3ddf1bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParameterizedModel(nn.Module):\n",
    "    \"\"\"Parameterized model constructor.\n",
    "    \n",
    "    Parameters:\n",
    "        vocab_size -- Size of the input vocabulary.\n",
    "        embed_size -- Feature size of the embedding layer.\n",
    "        recurrence_type -- pytorch class for the recurrence type (nn.RNN, nn.LSTM, nn.GRU, etc.)\n",
    "        recurrence_size -- Feature size of the recurrence layers.\n",
    "        num_recurrence -- Number of recurrence layers.\n",
    "        bidirectional -- Boolean indicating if the recurrence layers are bidirectional.\n",
    "        dropout -- Dropout value after each recurrence layer.\n",
    "        num_nonlinear -- Number of nonlinear layers. Valid range: 0-2\n",
    "        linear_size -- Hidden feature size of linear layers. Only applicable when num_nonlinear > 1.\n",
    "        output_size -- Final network output size.\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                 vocab_size, embed_size,\n",
    "                 recurrence_type, recurrence_size, num_recurrence, bidirectional,\n",
    "                 dropout,\n",
    "                 num_nonlinear, linear_size,\n",
    "                 output_size):\n",
    "        super(ParameterizedModel, self).__init__()\n",
    "\n",
    "        # Create embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        # Create recurrence layers\n",
    "        self.rnn = recurrence_type(embed_size, recurrence_size, num_recurrence, batch_first=True, dropout=dropout, bidirectional=bidirectional)\n",
    "        \n",
    "        # Convert bidirectionality to an integer value for later calculations.\n",
    "        self.num_directions = int(bidirectional == True) + 1\n",
    "\n",
    "        # Apply dropout after the final recurrence layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Create fully connected layers\n",
    "        if num_nonlinear == 0:\n",
    "            self.fc = nn.Sequential(nn.Linear(recurrence_size * self.num_directions, output_size))\n",
    "        elif num_nonlinear == 1:\n",
    "            self.fc = nn.Sequential(nn.ReLU(),\n",
    "                                    nn.Linear(recurrence_size * self.num_directions, output_size))\n",
    "        elif num_nonlinear == 2:\n",
    "            self.fc = nn.Sequential(nn.ReLU(),\n",
    "                                    nn.Linear(recurrence_size * self.num_directions, linear_size),\n",
    "                                    nn.ReLU(),\n",
    "                                    nn.Linear(linear_size, output_size))\n",
    "\n",
    "    def forward(self, x, state):\n",
    "        embed = self.embed(x)\n",
    "        out, state = self.rnn(embed, state)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        # out = self.linear(out[:, -1, :])\n",
    "        return out, state\n",
    "\n",
    "    def get_initial_state(self, batch_size, device):\n",
    "        if not isinstance(self.rnn, nn.LSTM):\n",
    "            # RNN & GRU take a tensor as hidden state\n",
    "            return torch.zeros(self.num_directions * self.rnn.num_layers,\n",
    "                               batch_size,\n",
    "                               self.rnn.hidden_size).to(device)\n",
    "        else:\n",
    "            # LSTM takes a tuple of hidden states\n",
    "            return (torch.zeros(self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size,\n",
    "                                self.rnn.hidden_size).to(device),\n",
    "                    torch.zeros(self.num_directions * self.rnn.num_layers,\n",
    "                                batch_size,\n",
    "                                self.rnn.hidden_size).to(device))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c651257-ea2a-4e7a-8ef4-1d5d9b38853e",
   "metadata": {},
   "source": [
    "# 5. Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c357dda6-aac8-4db5-931a-eddd2872d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, epochs, data_loader, device, optimizer, loss_function):\n",
    "    print(\"\\n -= Training Statistics =- \")\n",
    "    epoch_output = [\"Epoch,AverageLoss,AveragePerplexity,ElapsedTime\"]\n",
    "    start_time = time.perf_counter()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        epoch_perplexity = 0\n",
    "        state = None\n",
    "    \n",
    "        for batch_idx, (train_inputs, train_targets) in enumerate(data_loader):\n",
    "            train_inputs, train_targets = train_inputs.to(device), train_targets.to(device)\n",
    "            \n",
    "            # Detach states\n",
    "            if state is None:\n",
    "                state = model.get_initial_state(train_inputs.size(0), device)\n",
    "            else:\n",
    "                if not isinstance(state, tuple):\n",
    "                    #Detach state for RNN & GRU networks\n",
    "                    state.detach_()\n",
    "                else:\n",
    "                    # Detach all states for LSTM network\n",
    "                    for s in state:\n",
    "                        s.detach_()\n",
    "    \n",
    "            optimizer.zero_grad()\n",
    "            output, state = model(train_inputs, state)\n",
    "            loss = loss_function(output, train_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_perplexity += torch.exp(loss)\n",
    "\n",
    "        epoch_output.append(f\"{epoch + 1},{epoch_loss / len(data_loader) :.5f},{epoch_perplexity / len(data_loader) :.5f},{time.perf_counter() - start_time :.3f}\")\n",
    "        print(f'Epoch {epoch + 1} | Average loss: {epoch_loss / len(data_loader) :.3f} | Average perplexity: {epoch_perplexity / len(data_loader) :.3f} | Elapsed time: {time.perf_counter() - start_time :.3f} seconds')\n",
    "\n",
    "    return epoch_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909e32a8-d9d0-4027-a974-4ac7eefb574e",
   "metadata": {},
   "source": [
    "# 6. Text Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77f89bb5-b0fc-4fe7-a235-2ba79ff91c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_tensor(tokens, token_to_index):\n",
    "    token_indices = []\n",
    "    for token in tokens:\n",
    "        if token in token_to_index:\n",
    "            token_indices.append(token_to_index[token])\n",
    "        else:\n",
    "            # If the provided token isn't in the dictionary, choose a random one. Useful for word-based models.\n",
    "            token_indices.append(random.choice(list(token_to_index.values())))\n",
    "    # token_indices = [token_to_index[token] for token in tokens]\n",
    "    return torch.tensor(np.array([token_indices]), dtype=torch.long)\n",
    "\n",
    "def generate_text(model, seed, token_to_index, index_to_token, sequence_length, generate_length):\n",
    "    input_tensor = tokens_to_tensor(seed[-sequence_length:], token_to_index)\n",
    "    state = model.get_initial_state(batch_size=1, device=torch.device(\"cpu\"))\n",
    "\n",
    "    for _ in range(generate_length):\n",
    "        with torch.no_grad():\n",
    "            output, state = model(input_tensor, state)\n",
    "        predicted_index = torch.argmax(output, dim=1).item()\n",
    "        predicted_token = index_to_token[predicted_index]\n",
    "        seed.append(predicted_token)\n",
    "        input_tensor = tokens_to_tensor(seed[-sequence_length:], token_to_index)\n",
    "\n",
    "    return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e54917-fa4f-4a94-9f86-b2c0842317e3",
   "metadata": {},
   "source": [
    "# 7. Wrapper function to bring everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "567d31f1-4397-4b1b-a541-dadcc5196e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_generate(# Required parameters\n",
    "                       model_type,             # pytorch recurrent class for the model. Supported values: nn.RNN, nn.LSTM, nn.GRU\n",
    "                       use_words,              # If true, the model uses word tokens instead of character tokens\n",
    "                       seeds,                  # Array of text generation seed strings\n",
    "                       output_filename,         # Name for the output .csv, .txt, and .ckpt files\n",
    "                       \n",
    "                       # Data loading parameters\n",
    "                       char_limit = 10000,                       # Number of characters to load from the dataset\n",
    "                       line_offset = random.randrange(2900000),  # Number of lines (of ~3 million) to skip before reading data\n",
    "                       sequence_length = 30,                     # Length for training data token sequences\n",
    "                       batch_size = 32,                          # Batch size for training data\n",
    "                       \n",
    "                       # Model definition parameters\n",
    "                       embed_size = 256,       # Feature size of the embedding layer\n",
    "                       num_recurrence = 1,     # Number of recurrence layers\n",
    "                       recurrence_size = 512,  # Feature size of the recurrence layers\n",
    "                       bidirectional = False,  # If true, sets the recurrence layers to be bidirectional\n",
    "                       dropout = 0,            # Dropout after each recurrence layer\n",
    "                       num_nonlinear = 0,      # Number of nonlinear ReLU layers. Supported values: 0-2\n",
    "                       linear_size = 128,      # Feature size of linear layers. Only applies if num_nonlinear >= 2\n",
    "                       \n",
    "                       # Training parameters\n",
    "                       epochs = 20,            # Number of training epochs\n",
    "                       \n",
    "                       # Generation parameters\n",
    "                       generate_length = 500   # Number of tokens of output text to generate for each seed string\n",
    "                      ):\n",
    "    \n",
    "    vectorized_input, token_to_index, index_to_token = vectorize(data_file, char_limit, line_offset, use_words)\n",
    "    data_loader = construct_data_loader(vectorized_input, sequence_length, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    input_size = len(token_to_index)\n",
    "    output_size = len(token_to_index)\n",
    "\n",
    "    model = ParameterizedModel(input_size,\n",
    "                               embed_size,\n",
    "                               model_type,\n",
    "                               recurrence_size,\n",
    "                               num_recurrence,\n",
    "                               bidirectional,\n",
    "                               dropout,\n",
    "                               num_nonlinear,\n",
    "                               linear_size,\n",
    "                               output_size).to(device)\n",
    "    \n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "    epoch_output = train_model(model, epochs, data_loader, device, optimizer, loss_function)\n",
    "\n",
    "    print(\"\\n -= Generated Text =- \")\n",
    "    generated_text = \"\"\n",
    "\n",
    "    for seed in seeds:\n",
    "        if (use_words):\n",
    "            generated_text = generated_text + \"\\n\\n\" + \" \".join(generate_text(model,\n",
    "                                                                              seed.split(),\n",
    "                                                                              token_to_index,\n",
    "                                                                              index_to_token,\n",
    "                                                                              sequence_length,\n",
    "                                                                              generate_length))\n",
    "        else:\n",
    "            generated_text = generated_text + \"\\n\\n\" + \"\".join(generate_text(model,\n",
    "                                                                             list(seed),\n",
    "                                                                             token_to_index,\n",
    "                                                                             index_to_token,\n",
    "                                                                             sequence_length,\n",
    "                                                                             generate_length))\n",
    "\n",
    "    print(generated_text)\n",
    "\n",
    "    print(\"\\nSaving outputs and model...\")\n",
    "    \n",
    "    with open(f\"{output_filename}.csv\", \"w\") as csv:\n",
    "        csv.write('\\n'.join(epoch_output))\n",
    "\n",
    "    with open(f\"{output_filename}.txt\", \"w\") as txt:\n",
    "        txt.write(generated_text)\n",
    "\n",
    "    torch.save(model, f\"{output_filename}.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2639a3-b456-447e-aeb4-e18a9a19c656",
   "metadata": {},
   "source": [
    "# 8. Run the model for each model/token combination\n",
    "\n",
    "Output results can be found in the report .pdf file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cacef9b-e502-453f-835e-5787b070b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RNN model\n",
    "recurrence = nn.RNN\n",
    "use_words = False\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "output_filename = \"RNN_Test\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, output_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f46e15-e3ed-46d0-bacd-9b0b2a7f1d1a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Character-based RNN models\n",
    "recurrence = nn.RNN\n",
    "use_words = False\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 100000\n",
    "root_filename = \"RNN_char_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e75fd41-9761-4f15-b536-f9456e28b1c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word-based RNN models\n",
    "recurrence = nn.RNN\n",
    "use_words = True\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 200000\n",
    "root_filename = \"RNN_word_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2940b380-bb76-4cbb-8256-d8147816f3e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Character-based LSTM models\n",
    "recurrence = nn.LSTM\n",
    "use_words = False\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 100000\n",
    "root_filename = \"LSTM_char_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4d1522-f330-4f38-99ff-e44a5559d6b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word-based LSTM models\n",
    "recurrence = nn.LSTM\n",
    "use_words = True\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 200000\n",
    "root_filename = \"LSTM_word_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeba61b8-c21d-43e5-b3d4-c966f67807e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Character-based GRU models\n",
    "recurrence = nn.GRU\n",
    "use_words = False\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 100000\n",
    "root_filename = \"GRU_char_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0a6b22-6fc8-4f91-ac8a-c4930f1c7c0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Word-based GRU models\n",
    "recurrence = nn.GRU\n",
    "use_words = True\n",
    "seeds = [\"Two roads diverged in a yellow wood\",\n",
    "         \"And on the pedestal these words appear\",\n",
    "         \"Shall I compare thee to a summer's day?\"]\n",
    "char_limit = 200000\n",
    "root_filename = \"GRU_word_\"\n",
    "\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"base\", char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers2\", num_recurrence=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"layers3\", num_recurrence=3, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout20\", dropout=0.2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"dropout50\", dropout=0.5, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"bidirectional\", bidirectional=True, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear1\", num_nonlinear=1, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"nonlinear2\", num_nonlinear=2, char_limit=char_limit)\n",
    "train_and_generate(recurrence, use_words, seeds, root_filename + \"all\", num_recurrence=2, dropout=0.2, bidirectional=True, num_nonlinear=1, char_limit=char_limit)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
